## Autonomous_driving

- PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.pdf)
- DOES END-TO-END AUTONOMOUS DRIVING REALLY NEED PERCEPTION TASKS?  [paper](https://arxiv.org/pdf/2409.18341)
- HE-Drive: Human-Like End-to-End Driving with Vision Language Models [paper](https://arxiv.org/pdf/2410.05051)
- Truncated Diffusion Model for Real-Time End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2411.15139)
- Bridging Large Vision-Language Models and End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2411.15139)
- EMMA: End-to-End Multimodal Model for Autonomous Driving [paper](https://arxiv.org/abs/2410.23262)
- DriveMM: All-in-One Large Multimodal Model for Autonomous Driving [paper](https://arxiv.org/abs/2412.07689)
- Cosmos World Foundation Model Platform for Physical AI [paper](https://github.com/nvidia/cosmos)
- DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT [paper](https://arxiv.org/pdf/2412.19505v2)
- OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2412.15208)
- Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability [paper](https://arxiv.org/abs/2405.17398)
- Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2410.22313)
- DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model [paper](https://arxiv.org/pdf/2310.01412)
- DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2411.15139)
- GAIA-1: A Generative World Model for Autonomous Driving [paper](https://arxiv.org/abs/2309.17080)
- BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving [paper](https://arxiv.org/pdf/2503.03074)
- AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning [paper](https://arxiv.org/pdf/2503.07608)
- GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2503.05689)
- CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting [paper](https://arxiv.org/pdf/2503.07234)
- DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2503.07656)
- CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving [paper](https://arxiv.org/pdf/2503.08683)
- Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning [paper](https://arxiv.org/pdf/2503.14182)
- RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving [paper](https://arxiv.org/pdf/2503.13861)
- ChatBEV: A Visual Language Model that Understands BEV Maps [paper](https://arxiv.org/pdf/2503.13938)
- MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations [paper](https://arxiv.org/pdf/2503.13858)
- Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation [paper](https://arxiv.org/pdf/2503.12820)
- InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2503.13047)
- GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving [paper](https://arxiv.org/pdf/2503.20523)
- Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction [paper](https://www.arxiv.org/pdf/2503.11091)
- DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models [paper](https://arxiv.org/pdf/2503.11265)





  
## LLM
- DeepSeek-V3 [paper](https://arxiv.org/abs/2412.19437)
- DeepSeek-R1 [paper](DeepSeek-R1)
- Large Language Diffusion Models: [paper](https://arxiv.org/abs/2502.09992)
- s1: Simple test-time scaling: [paper](https://arxiv.org/abs/2501.19393)
- Gemma1~3: [Gemma3](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)
- Llama1~3:
- Gemini Embedding: Generalizable Embeddings from Gemini [paper](https://www.arxiv.org/pdf/2503.07891)
- Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models [paper](https://arxiv.org/pdf/2503.09573)
- LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens [paper](https://arxiv.org/pdf/2402.13753)
- LightThinker: Thinking Step-by-Step Compression [paper](https://arxiv.org/pdf/2502.15589)
- NeoBERT: A Next-Generation BERT [paper](https://arxiv.org/pdf/2502.19587v1)
- Thinking Slow, Fast:Scaling Inference Compute with Distilled Reasoners [paper](https://arxiv.org/pdf/2502.20339)
- Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers [paper](https://arxiv.org/pdf/2502.08145)
- QwQ-32B: Embracing the Power of Reinforcement Learning [blog](https://qwenlm.github.io/blog/qwq-32b/)
- Q-Filters: Leveraging Query-Key Geometry for Efficient Key-Value Cache Compression [paper](https://arxiv.org/pdf/2503.02812)
- START: Self-taught Reasoner with Tools [paper](https://arxiv.org/pdf/2503.04625)
- Transformers without Normalization [paper](https://arxiv.org/pdf/2503.10622)
- DAPO: an Open-Source LLM Reinforcement Learning System at Scale [paper](https://dapo-sia.github.io/static/pdf/dapo_paper.pdf)
- LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [paper](https://arxiv.org/pdf/2503.07536v1)
- RWKV-7 "Goose" with Expressive Dynamic State Evolution [paper](https://arxiv.org/pdf/2503.14456)
- SuperBPE: Space Travel for Language Models [paper](https://arxiv.org/pdf/2503.13423)
- NVIDIA Dynamo [github](https://github.com/ai-dynamo/dynamo)
- Qwen2.5-Omni [paper](https://arxiv.org/pdf/2503.20215)
- FFN Fusion: Rethinking Sequential Computation in Large Language Models [paper](https://arxiv.org/pdf/2503.18908)
- Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't [paper](https://arxiv.org/pdf/2503.16219)
- XAttention: Block Sparse Attention with Antidiagonal Scoring [paper](https://arxiv.org/pdf/2503.16428v1)


## VLM
- LLAVA-MINI: EFFICIENT IMAGE AND VIDEO LARGE MULTIMODAL MODELS WITH ONE VISION TOKEN [paper](https://arxiv.org/pdf/2501.03895v1)
- NVILA: Efficient Frontier Visual Language Models [paper](https://arxiv.org/pdf/2412.04468v1)
- Llama3.2-vision: [paper](https://arxiv.org/abs/2407.21783)
- SmolVLM: [blog](https://huggingface.co/blog/smolvlm)
- Phi-4: [paper](https://arxiv.org/abs/2503.01743)
- InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks [paper](https://arxiv.org/pdf/2312.14238)
- MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone [blog](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9)
- Instella-VL: [blog](https://rocm.blogs.amd.com/artificial-intelligence/Instella-BL-1B-VLM/README.html)
- Qwen-VL2.5: [paper](https://arxiv.org/abs/2502.13923)
- PaliGemma: A versatile 3B VLM for transfer [paper](https://arxiv.org/pdf/2407.07726)
- R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning [paper](https://arxiv.org/pdf/2503.05379)
- GRPO for vision - Teaching an LLM to reason about images [blog](https://www.groundlight.ai/blog/visual-reasoning-models)
- R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [paper](https://arxiv.org/pdf/2503.10615)
- SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion [paper](https://arxiv.org/pdf/2503.11576)
- NVIDIA Isaac GR00T N1: An Open Foundation Model for Humanoid Robots [paper](https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf)
- Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control [paper](https://arxiv.org/pdf/2503.14492)
- Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning [paper](https://arxiv.org/pdf/2503.18013)
- OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [paper](https://arxiv.org/pdf/2503.17352)
- Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning [paper](https://arxiv.org/pdf/2503.15558)
- Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control [paper](https://arxiv.org/pdf/2503.14492)
- R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization [paper](https://arxiv.org/pdf/2503.12937)

## VLA
-  π0: A Vision-Language-Action Flow Model for General Robot Control [paper](https://www.physicalintelligence.company/download/pi0.pdf)
-  FAST: Efficient Action Tokenization for Vision-Language-Action Models [paper](https://arxiv.org/pdf/2501.09747)
-  HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation [paper](https://arxiv.org/pdf/2502.05485)
-  DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control [paper](https://arxiv.org/pdf/2502.05855)
-  Survey on Vision-Language-Action Models [paper](https://arxiv.org/abs/2502.06851)
-  HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model [paper](https://arxiv.org/pdf/2503.10631)
-  Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills [paper](https://arxiv.org/abs/2503.12533)
-  GR00T N1: An Open Foundation Model for Generalist Humanoid Robots [paper](https://arxiv.org/pdf/2503.14734)
-  Gemini Robotics: Bringing AI into the Physical World [paper](https://arxiv.org/pdf/2503.20020)
-  CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models [paper[]()](https://arxiv.org/pdf/2503.22020) 
-  OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model [paper](https://arxiv.org/pdf/2503.23463)
-  π0.5: a Vision-Language-Action Model with Open-World Generalization[paper](https://www.physicalintelligence.company/download/pi05.pdf)
-  GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data [paper](https://arxiv.org/pdf/2505.03233)
-  OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation [paper](https://arxiv.org/pdf/2505.03912)
-  UniVLA: Learning to Act Anywhere with Task-centric Latent Actions [paper](https://arxiv.org/pdf/2505.06111)
-  VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation [paper](https://arxiv.org/pdf/2505.09577)
-  DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2505.16278)
-  BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization [paper](https://arxiv.org/pdf/2505.16640)
-  VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning [paper](https://arxiv.org/pdf/2505.18719)
-  DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving [paper](https://arxiv.org/pdf/2505.19381)
-  Hume: Introducing System-2 Thinking in Visual-Language-Action Model [paper](https://arxiv.org/pdf/2505.21432)
-  ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation [paper](https://arxiv.org/pdf/2505.22159)
-  Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models [paper](https://arxiv.org/pdf/2505.23757)
-  SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics [paper](https://arxiv.org/pdf/2506.01844)
-  Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning [paper](https://arxiv.org/pdf/2506.01953)
-  DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models [paper](https://arxiv.org/pdf/2506.05667)
-  BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation [paper](https://arxiv.org/pdf/2506.07530)
-  BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models [paper](https://arxiv.org/pdf/2506.07961)
-  EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models [paper](https://arxiv.org/pdf/2506.10100)
-  AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning [paper](https://arxiv.org/pdf/2506.13757)
-  WorldVLA: Towards Autoregressive Action World Model [paper](https://arxiv.org/pdf/2506.21539)
-  4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration [paper](https://arxiv.org/pdf/2506.22242)
-  Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding [paper](https://arxiv.org/pdf/2507.00416)
-  DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge [paper](https://arxiv.org/pdf/2507.04447)
-  EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos [paper](https://arxiv.org/pdf/2507.12440)
-  EdgeVLA: Efficient Vision-Language-Action Models [paper](https://arxiv.org/pdf/2507.14049)
-  GR-3 Technical Report [paper](https://arxiv.org/pdf/2507.15493)
-  FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning [paper](https://arxiv.org/pdf/2507.23318)
-  FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation [paper](https://arxiv.org/pdf/2508.02190)
-  MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming [paper](https://arxiv.org/pdf/2508.02549)
-  OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing [paper](https://arxiv.org/pdf/2508.08706)
-  ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver [paper](https://arxiv.org/pdf/2508.10333)
-  FlowVLA: Thinking in Motion with a Visual Chain of Thought [paper](https://arxiv.org/pdf/2508.18269)
-  MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation [paper](https://arxiv.org/pdf/2508.19236)
-  TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models [paper](https://arxiv.org/pdf/2508.19257)
-  Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation [paper](https://arxiv.org/pdf/2508.19958)
-  Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies [paper](https://arxiv.org/pdf/2508.20072)
-  CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification [paper](https://arxiv.org/pdf/2508.21046)
-  EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control [paper](https://arxiv.org/pdf/2508.21112)






## Knowledge Distillation
- Distillation Scaling Laws: [paper](https://arxiv.org/abs/2502.08606)


## Quantization
- AWQ: [paper](https://arxiv.org/abs/2306.00978)
- Matryoshka Quantization: [paper](https://arxiv.org/abs/2502.06786)
- Microscaling Data Formats: [paper](https://arxiv.org/abs/2310.10537)


- 
## 今後のToDo
- LLM, VLM調査
- RaspberryPiで自動運転AIモデル動かす
- Onnx, TensorRTのTensorRTの深堀
- CUDAプログラミング
- 量子化について調査
- ハード勉強(実際に、nanoとかでAIモデル動くように上記変換方法を行う)
