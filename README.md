## Autonomous_driving

- PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.pdf)
- DOES END-TO-END AUTONOMOUS DRIVING REALLY NEED PERCEPTION TASKS?  [paper](https://arxiv.org/pdf/2409.18341)
- HE-Drive: Human-Like End-to-End Driving with Vision Language Models [paper](https://arxiv.org/pdf/2410.05051)
- Truncated Diffusion Model for Real-Time End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2411.15139)
- Bridging Large Vision-Language Models and End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2411.15139)
- EMMA: End-to-End Multimodal Model for Autonomous Driving [paper](https://arxiv.org/abs/2410.23262)
- DriveMM: All-in-One Large Multimodal Model for Autonomous Driving [paper](https://arxiv.org/abs/2412.07689)
- Cosmos World Foundation Model Platform for Physical AI [paper](https://github.com/nvidia/cosmos)
- DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT [paper](https://arxiv.org/pdf/2412.19505v2)
- OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2412.15208)
- Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability [paper](https://arxiv.org/abs/2405.17398)
- Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving [paper](https://arxiv.org/abs/2410.22313)
- DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model [paper](https://arxiv.org/pdf/2310.01412)
- DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2411.15139)
- GAIA-1: A Generative World Model for Autonomous Driving [paper](https://arxiv.org/abs/2309.17080)
- BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving [paper]8https://arxiv.org/pdf/2503.03074)
- AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning [paper](https://arxiv.org/pdf/2503.07608)
- GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2503.05689)
- CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting [paper](https://arxiv.org/pdf/2503.07234)
- DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving [paper](https://arxiv.org/pdf/2503.07656)
- CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving [paper](https://arxiv.org/pdf/2503.08683)

  
## LLM
- DeepSeek-V3 [paper](https://arxiv.org/abs/2412.19437)
- DeepSeek-R1 [paper](DeepSeek-R1)
- Large Language Diffusion Models: [paper](https://arxiv.org/abs/2502.09992)
- s1: Simple test-time scaling: [paper](https://arxiv.org/abs/2501.19393)
- Gemma1~3: [Gemma3](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)
- Llama1~3:
- Gemini Embedding: Generalizable Embeddings from Gemini [paper](https://www.arxiv.org/pdf/2503.07891)
- Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models [paper](https://arxiv.org/pdf/2503.09573)
- LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens [paper](https://arxiv.org/pdf/2402.13753)
- LightThinker: Thinking Step-by-Step Compression [paper](https://arxiv.org/pdf/2502.15589)
- NeoBERT: A Next-Generation BERT [paper](https://arxiv.org/pdf/2502.19587v1)
- Thinking Slow, Fast:Scaling Inference Compute with Distilled Reasoners [paper](https://arxiv.org/pdf/2502.20339)
- Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers [paper](https://arxiv.org/pdf/2502.08145)
- QwQ-32B: Embracing the Power of Reinforcement Learning [blog](https://qwenlm.github.io/blog/qwq-32b/)
- Q-Filters: Leveraging Query-Key Geometry for Efficient Key-Value Cache Compression [paper](https://arxiv.org/pdf/2503.02812)
- START: Self-taught Reasoner with Tools [paper](https://arxiv.org/pdf/2503.04625)
- Transformers without Normalization [paper](https://arxiv.org/pdf/2503.10622)
- DAPO: an Open-Source LLM Reinforcement Learning System at Scale [paper](https://dapo-sia.github.io/static/pdf/dapo_paper.pdf)


## VLM
- LLAVA-MINI: EFFICIENT IMAGE AND VIDEO LARGE MULTIMODAL MODELS WITH ONE VISION TOKEN [paper](https://arxiv.org/pdf/2501.03895v1)
- NVILA: Efficient Frontier Visual Language Models [paper](https://arxiv.org/pdf/2412.04468v1)
- Llama3.2-vision: [paper](https://arxiv.org/abs/2407.21783)
- SmolVLM: [blog](https://huggingface.co/blog/smolvlm)
- Phi-4: [paper](https://arxiv.org/abs/2503.01743)
- InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks [paper](https://arxiv.org/pdf/2312.14238)
- MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone [blog](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9)
- Instella-VL: [blog](https://rocm.blogs.amd.com/artificial-intelligence/Instella-BL-1B-VLM/README.html)
- Qwen-VL2.5: [paper](https://arxiv.org/abs/2502.13923)
- PaliGemma: A versatile 3B VLM for transfer [paper](https://arxiv.org/pdf/2407.07726)
- R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning [paper](https://arxiv.org/pdf/2503.05379)
- GRPO for vision - Teaching an LLM to reason about images [blog](https://www.groundlight.ai/blog/visual-reasoning-models)
- R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [paper](https://arxiv.org/pdf/2503.10615)


## Knowledge Distillation
- Distillation Scaling Laws: [paper](https://arxiv.org/abs/2502.08606)


## Quantization
- AWQ: [paper](https://arxiv.org/abs/2306.00978)
- Matryoshka Quantization: [paper](https://arxiv.org/abs/2502.06786)
- Microscaling Data Formats: [paper](https://arxiv.org/abs/2310.10537)


- 
## 今後のToDo
- LLM, VLM調査
- RaspberryPiで自動運転AIモデル動かす
- Onnx, TensorRTのTensorRTの深堀
- CUDAプログラミング
- 量子化について調査
- ハード勉強(実際に、nanoとかでAIモデル動くように上記変換方法を行う)
